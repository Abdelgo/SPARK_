{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f52127-59aa-4098-9641-a3d02fe2cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark stream reading from the socket\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Sparkstream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bb844-a827-4b64-a718-01d101070ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparkstream\n",
      "You are working with 1 core(s)\n",
      "DataFrame[word: string]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# May take awhile locally\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # spark session definition\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"Sparkstream\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    # spark session info\n",
    "    cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "    print(spark.sparkContext.appName)\n",
    "    print(\"You are working with\", cores, \"core(s)\")\n",
    "    \n",
    "    \n",
    "    # stream input\n",
    "    streamtext = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").\\\n",
    "                            option(\"port\",8088).load()\n",
    "    print(streamtext.isStreaming)\n",
    "    # to lunch the socket lunch from terminal the command\n",
    "    # cat lorem.txt | nc -l 8088\n",
    "    \n",
    "    wordsdf = text.select(explode(split(streamtext.value,\" \")).alias(\"word\"))\n",
    "    print(wordsdf)\n",
    "    # wordsdf.show() # cannot be done because we are in streaming\n",
    "    \n",
    "    # streaming query\n",
    "    query = wordsdf.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "                    #append/complete etc console:to print result on the screen\n",
    "    \n",
    "    query.awaitTermination()\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf06db-afcc-4d5b-9101-7b7eca3cf2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Bigdata]",
   "language": "python",
   "name": "conda-env-Bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
